{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "tpnN_XZwq_vc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cancer Detection\n",
    "\n",
    "## Overview\n",
    "Convolutional neural networks are used to solve many computer vision problems. One of the more modern problems researchers and machine learning engineers have tried to solve is detecting cancer in cross-sectioned microscopic slides for various tissue types. These slide images are called \"whole slide images\" (WSI) they are *huge* images, usually 1-2 GBs large and are digitized with fancy scanner machines.\n",
    "\n",
    "Recently the FDA gave the first ever approval for a machine learning based digital pathology product [read more here](https://www.paige.ai/news/news-press-release1/). So the digital pathology industry is still growing and in need of great minds!\n",
    "\n",
    "### What you should already\n",
    "* Know the Python programming language and various packages like numpy and matplotlib\n",
    "* Be familiar with PyTorch\n",
    "* Have a basic understanding of machine and deep learning concepts\n",
    "\n",
    "### You will learn\n",
    "* To build a dense prediction model for image segmentation problems\n",
    "* To understand how to convert research papers into usable networks using PyTorch\n",
    "\n",
    "### Data set\n",
    "The data is given as a set of 1024×1024 PNG images. These images are \"tiles\" or small squares of the original WSI because working with one gigabit file wouldn't fit on most machines and would be extremely slow for training.\n",
    "Each input image (in the ```inputs``` directory) is an RGB image of a section of tissue,\n",
    "and there a file with the same name (in the ```outputs``` directory) \n",
    "that has a dense labeling of whether or not a section of tissue is cancerous\n",
    "(white pixels mean “cancerous”, while black pixels mean “not cancerous”).\n",
    "\n",
    "The data has been pre-split for into test and training splits.\n",
    "Filenames also reflect whether or not the image has any cancer at all \n",
    "(files starting with ```pos_``` have some cancerous pixels, while files \n",
    "starting with ```neg_``` have no cancer anywhere).\n",
    "All of the data is hand-labeled, so the dataset is not very large.\n",
    "This means that overfitting is a real possibility.\n",
    "\n",
    "An example image, and its corresponding ground truth labeling, is shown below.\n",
    "(And is contained in the downloadable dataset below).\n",
    "\n",
    "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=200&tok=a8ac31&media=cs501r_f2016:pos_test_000072_output.png)\n",
    "<img src=\"http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2016:pos_test_000072.png\" width=\"200\">\n",
    "\n",
    "### Articles & Papers to read beforehand\n",
    "* [U-Net: Convolutional Networks for Biomedical Image Segmentation | Arvix paper](https://arxiv.org/pdf/1505.04597.pdf)\n",
    "* [Up Sampling Images | Toward Data Science](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0)\n",
    "\n",
    "If you don't understand or are new to CNN's then chekcout this article as well\n",
    "* [Intro to Convolutional Neural Networks | Towards Data Science](https://towardsdatascience.com/an-introduction-to-convolutional-neural-networks-eb0b60b58fd7)\n",
    "\n",
    "___\n",
    "\n",
    "## Part 1 - Implement a dense image segmentation network\n",
    "\n",
    "#### 1.1 Dissecting the network topology\n",
    "\n",
    "Below is an image from the U-Net paper, it is an illustration of what a U-Net \"looks\" like. You can probably guess why they named it \"U\" net.\n",
    "\n",
    "![(Figure 1)](https://lh3.googleusercontent.com/qnHiB3B2KRxC3NjiSDtY08_DgDGTDsHcO6PP53oNRuct-p2QXCR-gyLkDveO850F2tTAhIOPC5Ha06NP9xq1JPsVAHlQ5UXA5V-9zkUrJHGhP_MNHFoRGnjBz1vn1p8P2rMWhlAb6HQ=w2400)\n",
    "\n",
    "Let's take a look at all the parts of this network illustration.\n",
    "\n",
    "We have, in order from the top-left of the \"U\", down to the bottom, and then up to the top-right of the \"U\":\n",
    "1) 3x3 convoultion followed by a ReLU\n",
    "2) 2x2 max pool\n",
    "3) 2x2 up convolution\n",
    "4) 1x1 final convolution\n",
    "5) \"Copy and crop\" for each layer\n",
    "\n",
    "TODO: dissect each part\n",
    "Let's dissect each part; its importance and what PyTorch module we will use to implement it\n",
    "\n",
    "TODO discuss layers\n",
    "\n",
    "#### Food for thought\n",
    "\n",
    "The simplest network you could implement (with all the desired properties)\n",
    "is just a single convolution layer with two filters and no relu! \n",
    "Why is that? (of course it wouldn't work very well!)\n",
    "\n",
    "\n",
    "#### 1.2 Creating the U-Net network"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below are the imports for all the packages and modules we will use. Run it before you run any other code."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, utils, datasets\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parameter import Parameter\n",
    "import pdb\n",
    "import torchvision\n",
    "import os\n",
    "import gzip\n",
    "import tarfile\n",
    "import gc\n",
    "from IPython.core.ultratb import AutoFormattedTB\n",
    "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
    "\n",
    "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch in /home/tanner/anaconda3/lib/python3.8/site-packages (1.7.0)\n",
      "Requirement already satisfied: future in /home/tanner/anaconda3/lib/python3.8/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /home/tanner/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /home/tanner/anaconda3/lib/python3.8/site-packages (from torch) (0.6)\n",
      "Requirement already satisfied: numpy in /home/tanner/anaconda3/lib/python3.8/site-packages (from torch) (1.19.2)\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQOefmcZVgTl",
    "pycharm": {
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class UNet(nn.module):\n",
    "    def __init__(self, in_channels, out_channels, filter_start_depth=64, u_depth=4):\n",
    "        super(UNet).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = u_depth\n",
    "        size = filter_start_depth\n",
    "\n",
    "        self.conv = nn.Conv2d\n",
    "        self.activation_func = nn.ReLU\n",
    "\n",
    "        self.down_convs = nn.ModuleList()\n",
    "        self.down_samples = nn.ModuleList()\n",
    "        self.up_samples = nn.ModuleList()\n",
    "        self.up_convs = nn.ModuleList()\n",
    "\n",
    "        previous_size = self.in_channels\n",
    "        current_size = size\n",
    "        # down the U\n",
    "        for i in range(self.depth) :\n",
    "            self.down_convs.append(nn.Sequential(\n",
    "                self.conv(previous_size, current_size, kernel_size=3, stride=1, padding=1),\n",
    "                self.activation_func(),\n",
    "                self.conv(current_size, current_size, kernel_size=3, stride=1, padding=1),\n",
    "                self.activation_func(),\n",
    "            ))\n",
    "            self.down_samples.append(nn.MaxPool2d(kernel_size=2))\n",
    "            previous_size = current_size\n",
    "            current_size *= 2\n",
    "\n",
    "        # bottom convolutions\n",
    "        self.bottom = nn.Sequential(\n",
    "            self.conv(previous_size, current_size, kernel_size=3, stride=1, padding=1),\n",
    "            self.activation_func(),\n",
    "            self.conv(current_size, current_size, kernel_size=3, stride=1, padding=1),\n",
    "            self.activation_func()\n",
    "        )\n",
    "\n",
    "        # up the U\n",
    "        for i in range(self.depth) :\n",
    "            next_size = current_size//2\n",
    "            self.up_samples.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(current_size, next_size, kernel_size=2, stride=2, padding=0),\n",
    "                self.activation_func(),\n",
    "            ))\n",
    "            self.up_convs.append(nn.Sequential(\n",
    "                self.conv(next_size, next_size, kernel_size=3, stride=1, padding=1),\n",
    "                self.activation_func(),\n",
    "                self.conv(next_size, next_size, kernel_size=3, stride=1, padding=1),\n",
    "                self.activation_func(),\n",
    "            ))\n",
    "            current_size = next_size\n",
    "\n",
    "        # last convolutional layer\n",
    "        self.final = self.conv(current_size, self.out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # go down the U\n",
    "        activations = []\n",
    "        for i in range(self.depth):\n",
    "            x = self.down_convs[i](x)\n",
    "            activations.append(x)\n",
    "            x = self.down_samples[i]()\n",
    "        \n",
    "        x = self.bottom(x)\n",
    "\n",
    "        # back up the U\n",
    "        for i in range(self.depth):\n",
    "            x = self.up_samples[i](x)\n",
    "            # figure out torch.cat\n",
    "            x = self.up_convs[i](x)\n",
    "\n",
    "        return self.final(x)\n",
    "\n",
    "def pad_to_shape(tensor, out_shape):\n",
    "    \"\"\"\n",
    "    Pads this image with zeroes to shp.\n",
    "    Args:\n",
    "        tensor: image tensor to pad\n",
    "        shp: desired output shape\n",
    "    Returns:\n",
    "        Zero-padded tensor of shape shp.\n",
    "    \"\"\"\n",
    "    if len(out_shape) == 4:\n",
    "        pad = (0, out_shape[3] - tensor.shape[3], 0, out_shape[2] - tensor.shape[2])\n",
    "    elif len(out_shape) == 5:\n",
    "        pad = (0, out_shape[4] - tensor.shape[4], 0, out_shape[3] - tensor.shape[3], 0, out_shape[2] - tensor.shape[2])\n",
    "    return F.pad(tensor, pad)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we wrap our image dataset into a PyTorch dataset object to make it easier to load the data into memory and create batches."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CancerDataset(Dataset):\n",
    "  def __init__(self, root, download=True, size=512, train=True):\n",
    "    if download and not os.path.exists(os.path.join(root, 'cancer_data')):\n",
    "      datasets.utils.download_url('http://liftothers.org/cancer_data.tar.gz', root, 'cancer_data.tar.gz', None)\n",
    "      self.extract_gzip(os.path.join(root, 'cancer_data.tar.gz'))\n",
    "      self.extract_tar(os.path.join(root, 'cancer_data.tar'))\n",
    "    \n",
    "    postfix = 'train' if train else 'test'\n",
    "    root = os.path.join(root, 'cancer_data', 'cancer_data')\n",
    "    self.dataset_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'inputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
    "    self.label_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'outputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
    "\n",
    "  @staticmethod\n",
    "  def extract_gzip(gzip_path, remove_finished=False):\n",
    "    print('Extracting {}'.format(gzip_path))\n",
    "    with open(gzip_path.replace('.gz', ''), 'wb') as out_f, gzip.GzipFile(gzip_path) as zip_f:\n",
    "      out_f.write(zip_f.read())\n",
    "    if remove_finished:\n",
    "      os.unlink(gzip_path)\n",
    "  \n",
    "  @staticmethod\n",
    "  def extract_tar(tar_path):\n",
    "    print('Untarring {}'.format(tar_path))\n",
    "    z = tarfile.TarFile(tar_path)\n",
    "    z.extractall(tar_path.replace('.tar', ''))\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    img = self.dataset_folder[index]\n",
    "    label = self.label_folder[index]\n",
    "    return img[0],label[0][0]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.dataset_folder)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Il_53HLSWPTY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SegmentationTrainer:\n",
    "    def __init__(self):\n",
    "        self.network = None\n",
    "        self.optimizer = None\n",
    "        self.loss_func = None\n",
    "        self.train_dataloader = None \n",
    "        self.val_dataloader = None\n",
    "\n",
    "        self.device = 'cuda'\n",
    "        self.epoch_count = 0\n",
    "        self.loop = None\n",
    "\n",
    "    def increment_epoch_count(self):\n",
    "        self.epoch_count += 1\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        for i in range(1, num_epochs+1):\n",
    "            self.loop = tqdm(total=len(self.train_dataloader), position=0, leave=False)\n",
    "            # Train\n",
    "            print(f'\\nTraining for epoch {i} of {num_epochs}')\n",
    "            train_loss, train_acc = self.train_epoch_(self.train_dataloader, training=True)\n",
    "            self.loop.close()\n",
    "\n",
    "            self.loop = tqdm(total=len(self.val_dataloader), position=0, leave=False)\n",
    "            # Validate\n",
    "            print(f\"\\nValidation for epoch {i} of {num_epochs}\")\n",
    "            val_loss, val_acc = self.train_epoch_(self.val_dataloader, training=False)\n",
    "            self.loop.close()\n",
    "            \n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "        return  train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "    def train_epoch_(self, data_loader, training=True):\n",
    "        if training:\n",
    "            self.network.train() \n",
    "            torch.set_grad_enabled(True)\n",
    "        else:\n",
    "            self.network.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "\n",
    "        loss_sum = 0\n",
    "        acc_sum = 0\n",
    "        for imgs, labels in data_loader:\n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device,non_blocking=True) # non_blocking is a speed up (async)\n",
    "            \n",
    "            if training:\n",
    "                self.optimizer.zero_grad() # Set gradient to zero\n",
    "\n",
    "            y_hat = self.network(imgs)\n",
    "            loss = self.loss_func(y_hat, labels.long())\n",
    "            loss_sum += loss.detach().sum().item()\n",
    "\n",
    "            \n",
    "            probs = y_hat.argmax(1)\n",
    "            accuracy = (probs == labels).float().mean()\n",
    "            acc_sum += accuracy.detach().item()\n",
    "\n",
    "            mem_allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "\n",
    "            self.loop.set_description('loss: {:.4f}, accuracy: {:.4f}, mem: {:.2f}'.format(loss.detach().sum().item(), accuracy, mem_allocated))\n",
    "            self.loop.update(1)\n",
    "\n",
    "            if training:\n",
    "                loss.backward() # Compute gradient, for weight with respect to loss\n",
    "                self.optimizer.step() # Take step in the direction of the negative gradient\n",
    "\n",
    "        return loss_sum / len(data_loader.dataset), acc_sum / len(data_loader.dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we instantiate anything we need for training. Datasets, the network, loss functions, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Instantiate data sets\n",
    "train_dataset = CancerDataset('/tmp', train=True)\n",
    "val_dataset = CancerDataset('/tmp', train=False)\n",
    "\n",
    "# Instantiate data loaders\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=4,\n",
    "                          num_workers=4,\n",
    "                          pin_memory=True,)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=4,\n",
    "                        num_workers=4,\n",
    "                        pin_memory=True)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAjagHCdGNAh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Instantiate the network\n",
    "device = \"cuda\"\n",
    "\n",
    "net = UNet(\n",
    "    3,\n",
    "    2,\n",
    "    filter_start_depth=64,\n",
    "    u_depth=4\n",
    ").to(device)\n",
    "\n",
    "# Instantiate loss function and optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr = learning_rate\n",
    ")\n",
    "loss_func = nn.CrossEntropyLoss().to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we will create a training loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is what was talked about in the video for memory management\n",
    "\n",
    "def scope():\n",
    "  try:\n",
    "    #your code for calling dataset and dataloader\n",
    "    gc.collect()\n",
    "    print(torch.cuda.memory_allocated() / 1e9)\n",
    "    \n",
    "    #for epochs:\n",
    "    # Call your model, figure out loss and accuracy\n",
    "    \n",
    "  except:\n",
    "    __ITB__()\n",
    "    \n",
    "scope()\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RkieTbwlYWPS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "WARNING: You may run into an error that says \"RuntimeError: CUDA out of memory.\"\n",
    "\n",
    "In this case, the memory required for your batch is larger than what the GPU is capable of. You can solve this problem by adjusting the image size or the batch size and then restarting the runtime. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "___\n",
    "\n",
    "## Part 2 - Display performance\n",
    "\n",
    "Plot performance over time\n",
    "\n",
    "Please generate two plots:\n",
    "\n",
    " One that shows loss on the training and validation set as a function of training time. \n",
    "\n",
    " One that shows accuracy on the training and validation set as a function of training time. \n",
    "\n",
    " Make sure your axes are labeled!\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "CZ062Jv1jIIu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Your plotting code here\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTg1jyIsYVZN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NOTE:**\n",
    "\n",
    "Guessing that the pixel is not cancerous every single time will give you an accuracy of ~ 85%.\n",
    "Your trained network should be able to do better than that (but you will not be graded on accuracy).\n",
    "This is the result I got after 1 hour or training.\n",
    "\n",
    "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=d23e0b&media=cs501r_f2016:training_accuracy.png)\n",
    "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=bb8e3c&media=cs501r_f2016:training_loss.png)"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "\n",
    "## Part 3 - Generating predictions\n",
    "\n",
    "This is the real test to see how well your model accomplished its task. We need it to work outside of a training environment and in the real world with novel data points.\n",
    "\n",
    "Generate at least 5 predictions on the pos_test_000072.png image and display them as images. These predictions should be made at a reasonable interval (e.g. every epoch). \n",
    "\n",
    "You can load this image from the file pos_test_000072.png, or you can get it from the dataset object. It is item 172 of the validation dataset.\n",
    "You can print both the data instance (x) and the ground-truth label (y_hat) to see how well your network predicts on that instance.\n",
    "\n",
    "To do this, calculate the output of your trained network on the pos_test_000072.png image,\n",
    "then make a hard decision (cancerous/not-cancerous) for each pixel.\n",
    "The resulting image should be black-and-white, where white pixels represent areas\n",
    "the network considers cancerous."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "S4s92S2_jQOG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Code for testing prediction on an image\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXfG3wClh8an",
    "pycharm": {
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "interpreter": {
   "hash": "de0ed7c7f898e567d4d5b9b213d22f40d2a2eccb125a7b6be0e0e1c3ce1f2abf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}